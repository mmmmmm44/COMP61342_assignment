{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5df4db",
   "metadata": {},
   "source": [
    "Inference with the validation set (as I forgot to save the result of the evaluation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e8b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "from helper import print_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbee4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2017 = 'train2017'\n",
    "val2017 = 'val2017'\n",
    "ann_file = 'dataset/coco/annotations/instances_{}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e7b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_10_CATS_ID = set([1,  3, 62, 84, 44, 47, 67, 51, 10, 31])\n",
    "CATS_NAMES = {\n",
    "    1: 'person',\n",
    "    3: 'car',\n",
    "    62: 'chair',\n",
    "    84: 'book',\n",
    "    44: 'bottle',\n",
    "    47: 'cup',\n",
    "    67: 'dinning table',\n",
    "    51: 'traffic light',\n",
    "    10: 'bowl',\n",
    "    31: 'handbag'\n",
    "}\n",
    "LABELS = [CATS_NAMES[id] for id in sorted(list(TOP_10_CATS_ID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9c1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=7.41s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = COCO(ann_file.format(train2017))\n",
    "coco_val = COCO(ann_file.format(val2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd848eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_images_and_labels(coco):\n",
    "\n",
    "    # get all filenames\n",
    "    img_ids_w_filename = {coco.dataset['images'][i]['id']: coco.dataset['images'][i]['file_name'] for i in range(len(coco.dataset['images']))}      # use dictionary for faster query\n",
    "\n",
    "    # get all images\n",
    "    img_ids = [coco.dataset['images'][i]['id'] for i in range(len(coco.dataset['images']))]\n",
    "\n",
    "    # load labels for each imgs (as one img may have multiple labels)\n",
    "    labels_per_imgs = []\n",
    "    for i in range(len(img_ids)):\n",
    "        labels_per_imgs.append(coco.loadAnns(coco.getAnnIds(imgIds=img_ids[i])))\n",
    "\n",
    "    img_id_w_bb = []\n",
    "    label_per_obj = []\n",
    "\n",
    "    for labels in labels_per_imgs:\n",
    "        for l in labels:\n",
    "            img_id_w_bb.append((l['id'], l['image_id'], l['bbox']))\n",
    "            label_per_obj.append(l['category_id'])\n",
    "\n",
    "    return img_ids_w_filename, img_id_w_bb, label_per_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2696153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_w_filename_train, img_id_w_bb_train, label_per_obj_train = get_coco_images_and_labels(coco_train)\n",
    "img_ids_w_filename_val, img_id_w_bb_val, label_per_obj_val = get_coco_images_and_labels(coco_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf71d36",
   "metadata": {},
   "source": [
    "Dataset save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb14495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filtered dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "filtered_dataset_dir = Path('dataset/coco_top10_filtered_20250423')\n",
    "\n",
    "# with open(filtered_dataset_dir / 'img_id_w_bb_train_top10_v2.pkl', 'rb') as f:\n",
    "#     img_id_w_bb_train_top10_filtered = pickle.load(f)\n",
    "# with open(filtered_dataset_dir / 'label_per_obj_train_top10_v2.pkl', 'rb') as f:\n",
    "#     label_per_obj_train_top10_filtered = pickle.load(f)\n",
    "\n",
    "with open(filtered_dataset_dir/ 'img_id_w_bb_val_top10.pkl', 'rb') as f:\n",
    "    img_id_w_bb_val_top10 = pickle.load(f)\n",
    "with open(filtered_dataset_dir / 'label_per_obj_val_top10.pkl', 'rb') as f:\n",
    "    label_per_obj_val_top10 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e91ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(img_id_w_bb_train_top10_filtered), len(label_per_obj_train_top10_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a48ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20312, 20312)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_id_w_bb_val_top10), len(label_per_obj_val_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6094a1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc54f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 1. First, prepare your data for the datasets library\n",
    "def prepare_dataset_dict(X, img_ids_w_filename):\n",
    "    # Create a lightweight dictionary containing only metadata (not images)\n",
    "    dataset_dict = {\n",
    "        \"image_id\": [],\n",
    "        \"bbox\": [],\n",
    "        \"file_name\": []\n",
    "    }\n",
    "    \n",
    "    for sample in X:\n",
    "        dataset_dict[\"image_id\"].append(sample[1])\n",
    "        dataset_dict[\"bbox\"].append(sample[2])\n",
    "        dataset_dict[\"file_name\"].append(img_ids_w_filename[sample[1]])\n",
    "        \n",
    "    return dataset_dict\n",
    "\n",
    "# 2. Define the SIFT processing function\n",
    "\n",
    "def process_image_with_sift(example, coco_ds):\n",
    "    \"\"\"Process a single image, extracting SIFT features\"\"\"\n",
    "    # Load image only when needed\n",
    "    img_path = Path(f\"dataset/coco/{coco_ds}/{example['file_name']}\")\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    # Apply bounding box\n",
    "    x, y, w, h = example['bbox']\n",
    "    img_cropped = img[int(y): int(y + h) + 1, int(x):int(x + w) + 1]\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img_gray = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply SIFT\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_gray, None)\n",
    "    \n",
    "    # Convert keypoints to serializable format\n",
    "    serialized_keypoints = []\n",
    "    for kp in keypoints:\n",
    "        serialized_keypoints.append({\n",
    "            'x': float(kp.pt[0]),\n",
    "            'y': float(kp.pt[1]), \n",
    "            'size': float(kp.size),\n",
    "            'angle': float(kp.angle),\n",
    "            'response': float(kp.response),\n",
    "            'octave': int(kp.octave)\n",
    "        })\n",
    "    \n",
    "    # Return only the features, together with the image_id, bbox, and filename\n",
    "    # but not the image (pixel) itself\n",
    "    return {\n",
    "        'image_id': example['image_id'],\n",
    "        'bbox': example['bbox'],\n",
    "        'file_name': example['file_name'],\n",
    "        'keypoints': serialized_keypoints,\n",
    "        'descriptors': descriptors.tolist() if descriptors is not None else []\n",
    "    }\n",
    "\n",
    "# 3. Main pipeline\n",
    "def create_sift_dataset(X_train, coco_ds, img_ids_w_filename):\n",
    "    # Create the dataset dictionary\n",
    "    dataset_dict = prepare_dataset_dict(X_train, img_ids_w_filename)\n",
    "    \n",
    "    # Create HF dataset\n",
    "    raw_dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Process using (optional) multi-processing\n",
    "    processed_dataset = raw_dataset.map(\n",
    "        process_image_with_sift,\n",
    "        fn_kwargs={'coco_ds': coco_ds},\n",
    "        num_proc=2,\n",
    "        batched=False,\n",
    "        desc=\"Extracting SIFT features\"\n",
    "    )\n",
    "    \n",
    "    return processed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ea746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset already exists. Loading from disk...\n"
     ]
    }
   ],
   "source": [
    "sift_dataset_val_path = Path('dataset/coco_top10_filtered_20250423/sift_dataset_val')\n",
    "if sift_dataset_val_path.exists():\n",
    "    sift_dataset_val = Dataset.load_from_disk(sift_dataset_val_path)\n",
    "\n",
    "    print(\"Validation dataset already exists. Loading from disk...\")\n",
    "else:\n",
    "    # create the dataset\n",
    "    sift_dataset_val = create_sift_dataset(img_id_w_bb_val_top10, val2017, img_ids_w_filename_val)\n",
    "\n",
    "    # try to save the dataset\n",
    "    sift_dataset_val.save_to_disk('dataset/coco_top10_filtered_20250423/sift_dataset_val')\n",
    "\n",
    "    print(\"Validation dataset is created and saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a919526",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6beab344",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCIPTORS_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ccccd",
   "metadata": {},
   "source": [
    "Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21695e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "K_GRID = [50, 100, 150, 200]\n",
    "PCA_N_COMPONENTS_GRID = [20, 50, 128]       # 128 is the default for SIFT -> no PCA reduction\n",
    "\n",
    "hyperparam_comb = list(product(K_GRID, PCA_N_COMPONENTS_GRID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db7df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tdy = datetime(2025, 4, 23, 22, 57, 33)\n",
    "top_model_dir = Path(f'models/PCA-SIFT/{tdy.strftime(\"%Y%m%d-%H%M%S\")}/')\n",
    "if not top_model_dir.exists():\n",
    "    top_model_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04c47c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation test-set function\n",
    "\n",
    "# extend the dataset with the cluster id (equivalent to vector quantization)\n",
    "def assign_cluster_id(example, pca, kmeans, pca_n_components):\n",
    "    \"\"\"Assign cluster id to each keypoint based on the closest cluster center\"\"\"\n",
    "    # Update the example with the cluster ids\n",
    "\n",
    "    des = np.array(example['descriptors'])\n",
    "    # check if descriptors are empty\n",
    "    if des.size == 0:\n",
    "        example['cluster_ids'] = np.array([], dtype=np.int32)\n",
    "        return example\n",
    "    \n",
    "    # apply PCA to the descriptors\n",
    "    if pca_n_components < DESCIPTORS_DIM:\n",
    "        red_des = pca.transform(des)\n",
    "    else:\n",
    "        red_des = des\n",
    "\n",
    "    # early return if transformed descriptors are empty\n",
    "    if red_des.size == 0:\n",
    "        example['cluster_ids'] = np.array([], dtype=np.int32)\n",
    "        return example\n",
    "\n",
    "    if red_des.ndim == 1:\n",
    "        example['cluster_ids'] = kmeans.predict(red_des.reshape(1, -1))\n",
    "    else:\n",
    "        example['cluster_ids'] = kmeans.predict(red_des)\n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fae8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the cluster ids\n",
    "# that will be used to compute TF-IDF\n",
    "\n",
    "def create_histogram(example, K):\n",
    "    \"\"\"Create a histogram of cluster ids\"\"\"\n",
    "\n",
    "    # early exit if descriptors are empty -> cluster_ids will be empty too\n",
    "    if len(example['descriptors']) == 0:\n",
    "        example['histogram'] = np.array([[]], dtype=np.int64)\n",
    "        return example\n",
    "\n",
    "    hist, _ = np.histogram(example['cluster_ids'], bins=np.arange(K + 1))\n",
    "    \n",
    "    example['histogram'] = hist.reshape(-1, K)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98992477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation sub-functions\n",
    "# grab accuracy score, confusion matrix, and classification report\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    \"\"\"Compute accuracy score\"\"\"\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def compute_classification_report(y_true, y_pred, labels):\n",
    "    \"\"\"Compute classification report\"\"\"\n",
    "    return classification_report(y_true, y_pred, target_names=labels, zero_division=0)\n",
    "\n",
    "def compute_confusion_matrix(y_true, y_pred, labels, save=False, save_path=None):\n",
    "    \"\"\"Compute confusion matrix\"\"\"\n",
    "    cm_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm_matrix, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    cm_disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=90)\n",
    "\n",
    "    if save:\n",
    "        if save_path is None:\n",
    "            raise ValueError(\"save_path must be provided if save is True\")\n",
    "        plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b571efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sift_dataset, y, pca, kmeans, tfidf, svm, K, PCA_N_COMPONENT):\n",
    "    sift_dataset = sift_dataset.map(\n",
    "        assign_cluster_id,\n",
    "        fn_kwargs={'kmeans': kmeans, 'pca': pca, 'pca_n_components': PCA_N_COMPONENT},\n",
    "        num_proc=1,\n",
    "        desc=\"Assigning cluster ids to keypoints. Including PCA -> KMeans\"\n",
    "    )\n",
    "\n",
    "    sift_dataset = sift_dataset.map(\n",
    "        create_histogram,\n",
    "        fn_kwargs={'K': K},\n",
    "        num_proc=1,\n",
    "        desc=\"Creating histogram of cluster ids.\"\n",
    "    )\n",
    "\n",
    "    des_histo = np.concatenate(\n",
    "        [example['histogram'] for example in sift_dataset if len(example['histogram'][0]) > 0],\n",
    "        axis=0\n",
    "    )\n",
    "    des_histo = des_histo.reshape(des_histo.shape[0], -1)\n",
    "\n",
    "    # Convert the list of descriptors to TF-IDF representation\n",
    "    tfidf_matrix = tfidf.transform(des_histo)\n",
    "\n",
    "    y_filtered = [y[i] for i, example in enumerate(sift_dataset) if len(example['histogram'][0]) > 0]\n",
    "    y_filtered = np.array(y_filtered)\n",
    "\n",
    "    # Predict the labels using the SVM model\n",
    "    y_pred = svm.predict(tfidf_matrix)\n",
    "\n",
    "    return y_filtered, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8274598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluations(y, y_pred, labels, model_dir, eval_stage=str):\n",
    "    \"\"\"Save the evaluation results\n",
    "    \n",
    "    eval_stage: str\n",
    "        The stage of the evaluation. It can be 'train', 'test' or 'val.\n",
    "    \"\"\"\n",
    "    # Save the accuracy score\n",
    "    accuracy = compute_accuracy(y, y_pred)\n",
    "    print(f\"Accuracy [{eval_stage}]: {accuracy}\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report = compute_classification_report(y, y_pred, labels)\n",
    "    with open(model_dir / f'classification_report_{eval_stage}.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    cm_path = model_dir / f'confusion_matrix_{eval_stage}.png'\n",
    "    compute_confusion_matrix(y, y_pred, labels, save=True, save_path=cm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1962eb4",
   "metadata": {},
   "source": [
    "hyperparameter re-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29cf254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - K: 50, PCA_N_COMPONENT: 20\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - Files already exist. Skipping evaluation for K=50 and PCA_N_COMPONENT=20.\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - K: 50, PCA_N_COMPONENT: 50\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - Files already exist. Skipping evaluation for K=50 and PCA_N_COMPONENT=50.\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - K: 50, PCA_N_COMPONENT: 128\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - Files already exist. Skipping evaluation for K=50 and PCA_N_COMPONENT=128.\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - K: 100, PCA_N_COMPONENT: 20\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - Files already exist. Skipping evaluation for K=100 and PCA_N_COMPONENT=20.\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:685] - K: 100, PCA_N_COMPONENT: 50\n",
      "[2025-04-25 01:00:25:685] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=100 and PCA_N_COMPONENT=50.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 100, PCA_N_COMPONENT: 128\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=100 and PCA_N_COMPONENT=128.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 150, PCA_N_COMPONENT: 20\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=150 and PCA_N_COMPONENT=20.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 150, PCA_N_COMPONENT: 50\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=150 and PCA_N_COMPONENT=50.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 150, PCA_N_COMPONENT: 128\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=150 and PCA_N_COMPONENT=128.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 200, PCA_N_COMPONENT: 20\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=200 and PCA_N_COMPONENT=20.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 200, PCA_N_COMPONENT: 50\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=200 and PCA_N_COMPONENT=50.\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - K: 200, PCA_N_COMPONENT: 128\n",
      "[2025-04-25 01:00:25:686] - --------------------------------------------------\n",
      "[2025-04-25 01:00:25:686] - Files already exist. Skipping evaluation for K=200 and PCA_N_COMPONENT=128.\n"
     ]
    }
   ],
   "source": [
    "for i, (K, PCA_N_COMPONENT) in enumerate(hyperparam_comb):\n",
    "    print_log(f'-' * 50)\n",
    "    print_log(f'K: {K}, PCA_N_COMPONENT: {PCA_N_COMPONENT}')\n",
    "    print_log(f'-' * 50)\n",
    "\n",
    "    # create a directory for each model\n",
    "    model_dir = top_model_dir / f'KMeans_{K}_PCA_{PCA_N_COMPONENT}'\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir(parents=True)\n",
    "        print_log(f'Model directory {model_dir} created.')\n",
    "    \n",
    "    # check if the files already exist\n",
    "    y_val_filtered_path = model_dir / f'y_val_filtered_{K}_{PCA_N_COMPONENT}.npy'\n",
    "    y_pred_val_path = model_dir / f'y_pred_val_{K}_{PCA_N_COMPONENT}.npy'\n",
    "    if y_val_filtered_path.exists() and y_pred_val_path.exists():\n",
    "        print_log(f'Files already exist. Skipping evaluation for K={K} and PCA_N_COMPONENT={PCA_N_COMPONENT}.')\n",
    "        continue\n",
    "\n",
    "\n",
    "    pca_model_name = f'PCA-SIFT_PCA_PCA-N_{PCA_N_COMPONENT}' + '.pkl'\n",
    "    kmeans_model_name = f'PCA-SIFT_KMeans_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "    tfidf_model_name = f'PCA-SIFT_TFIDF_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "    svm_model_name = f'PCA-SIFT_SVM-SGD_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}.pkl'\n",
    "\n",
    "    # load the models\n",
    "    if (model_dir / pca_model_name).exists():\n",
    "        pca = pickle.load(open(model_dir / pca_model_name, 'rb'))\n",
    "    else:\n",
    "        pca = None\n",
    "\n",
    "    kmeans = pickle.load(open(model_dir / kmeans_model_name, 'rb'))\n",
    "    tfidf = pickle.load(open(model_dir / tfidf_model_name, 'rb'))\n",
    "    svm_sgd = pickle.load(open(model_dir / svm_model_name, 'rb'))\n",
    "\n",
    "\n",
    "    # evaluate on validation set\n",
    "    y_val_filtered, y_pred_val = evaluate_model(sift_dataset_val, label_per_obj_val_top10, pca, kmeans, tfidf, svm_sgd, K, PCA_N_COMPONENT)\n",
    "    save_evaluations(y_val_filtered, y_pred_val, labels=LABELS,  model_dir=model_dir, eval_stage='val')\n",
    "    print_log(f'Evaluation on validation set done.')\n",
    "\n",
    "    # save the prediction result for future use (create further evaluation)\n",
    "    np.save(y_val_filtered_path, y_val_filtered)\n",
    "    np.save(y_pred_val_path, y_pred_val)\n",
    "\n",
    "    print_log(f'Finished evaluation for K={K} and PCA_N_COMPONENT={PCA_N_COMPONENT}.')\n",
    "    print_log(f'-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46dfe3",
   "metadata": {},
   "source": [
    "Single evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1ca23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "PCA_N_COMPONENT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddbd4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for each model\n",
    "model_dir = top_model_dir / f'KMeans_{K}_PCA_{PCA_N_COMPONENT}'\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True)\n",
    "    print_log(f'Model directory {model_dir} created.')\n",
    "\n",
    "pca_model_name = f'PCA-SIFT_PCA_PCA-N_{PCA_N_COMPONENT}' + '.pkl'\n",
    "kmeans_model_name = f'PCA-SIFT_KMeans_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "tfidf_model_name = f'PCA-SIFT_TFIDF_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "svm_model_name = f'PCA-SIFT_SVM-SGD_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}.pkl'\n",
    "\n",
    "# load the models\n",
    "if (model_dir / pca_model_name).exists():\n",
    "    pca = pickle.load(open(model_dir / pca_model_name, 'rb'))\n",
    "else:\n",
    "    pca = None\n",
    "\n",
    "kmeans = pickle.load(open(model_dir / kmeans_model_name, 'rb'))\n",
    "tfidf = pickle.load(open(model_dir / tfidf_model_name, 'rb'))\n",
    "svm_sgd = pickle.load(open(model_dir / svm_model_name, 'rb'))\n",
    "\n",
    "\n",
    "# evaluate on validation set\n",
    "y_val_filtered, y_pred_val = evaluate_model(sift_dataset_val, label_per_obj_val_top10, pca, kmeans, tfidf, svm_sgd, K, PCA_N_COMPONENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28fa8ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.4482640784237015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "f1 = f1_score(y_val_filtered, y_pred_val, average='weighted')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a248d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person': {'precision': 0.6419445310065441,\n",
       "  'recall': 0.81030583144852,\n",
       "  'f1-score': 0.7163660073896979,\n",
       "  'support': 10169.0},\n",
       " 'car': {'precision': 0.1933292155651637,\n",
       "  'recall': 0.18810096153846154,\n",
       "  'f1-score': 0.1906792567773378,\n",
       "  'support': 1664.0},\n",
       " 'bowl': {'precision': 0.15358361774744028,\n",
       "  'recall': 0.19607843137254902,\n",
       "  'f1-score': 0.1722488038277512,\n",
       "  'support': 459.0},\n",
       " 'handbag': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 463.0},\n",
       " 'bottle': {'precision': 0.07848101265822785,\n",
       "  'recall': 0.03311965811965812,\n",
       "  'f1-score': 0.04658151765589782,\n",
       "  'support': 936.0},\n",
       " 'cup': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 826.0},\n",
       " 'traffic light': {'precision': 0.09815950920245399,\n",
       "  'recall': 0.02735042735042735,\n",
       "  'f1-score': 0.0427807486631016,\n",
       "  'support': 585.0},\n",
       " 'chair': {'precision': 0.22476190476190477,\n",
       "  'recall': 0.07181984175289105,\n",
       "  'f1-score': 0.1088560885608856,\n",
       "  'support': 1643.0},\n",
       " 'dinning table': {'precision': 0.03067484662576687,\n",
       "  'recall': 0.007462686567164179,\n",
       "  'f1-score': 0.012004801920768308,\n",
       "  'support': 670.0},\n",
       " 'book': {'precision': 0.21596009975062344,\n",
       "  'recall': 0.4926052332195677,\n",
       "  'f1-score': 0.3002773925104022,\n",
       "  'support': 879.0},\n",
       " 'accuracy': 0.5054116103640538,\n",
       " 'macro avg': {'precision': 0.1636894737318125,\n",
       "  'recall': 0.18268430713692388,\n",
       "  'f1-score': 0.15897946173058425,\n",
       "  'support': 18294.0},\n",
       " 'weighted avg': {'precision': 0.41711353756572994,\n",
       "  'recall': 0.5054116103640538,\n",
       "  'f1-score': 0.4482640784237015,\n",
       "  'support': 18294.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_val_filtered, y_pred_val, target_names=LABELS, zero_division=0, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f5718e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 67,  1, ..., 47,  1,  1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7772668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp61342_asm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
