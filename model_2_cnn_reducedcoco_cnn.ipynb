{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pre-trained image classification models from pytorch  \n",
    "Fine-tuned on our reduced coco-dataset (for fair comparison)\n",
    "\n",
    "Reference: \n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from helper import print_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2017 = 'train2017'\n",
    "val2017 = 'val2017'\n",
    "ann_file = 'dataset/coco/annotations/instances_{}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_10_CATS_ID = set([1,  3, 62, 84, 44, 47, 67, 51, 10, 31])\n",
    "CATS_NAMES = {\n",
    "    1: 'person',\n",
    "    3: 'car',\n",
    "    62: 'chair',\n",
    "    84: 'book',\n",
    "    44: 'bottle',\n",
    "    47: 'cup',\n",
    "    67: 'dinning table',\n",
    "    51: 'traffic light',\n",
    "    10: 'bowl',\n",
    "    31: 'handbag'\n",
    "}\n",
    "LABEL_LOGITS_MAPPING = {\n",
    "    1: 0,\n",
    "    3: 1,\n",
    "    62: 2,\n",
    "    84: 3,\n",
    "    44: 4,\n",
    "    47: 5,\n",
    "    67: 6,\n",
    "    51: 7,\n",
    "    10: 8,\n",
    "    31: 9\n",
    "}\n",
    "LOGITS_LABEL_MAPPING = {v:k for k, v in LABEL_LOGITS_MAPPING.items()}\n",
    "LABELS = [CATS_NAMES[k] for k in LABEL_LOGITS_MAPPING.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=6.49s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.90s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = COCO(ann_file.format(train2017))\n",
    "coco_val = COCO(ann_file.format(val2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_images_and_labels(coco):\n",
    "\n",
    "    # get all filenames\n",
    "    img_ids_w_filename = {coco.dataset['images'][i]['id']: coco.dataset['images'][i]['file_name'] for i in range(len(coco.dataset['images']))}      # use dictionary for faster query\n",
    "\n",
    "    # get all images\n",
    "    img_ids = [coco.dataset['images'][i]['id'] for i in range(len(coco.dataset['images']))]\n",
    "\n",
    "    # load labels for each imgs (as one img may have multiple labels)\n",
    "    labels_per_imgs = []\n",
    "    for i in range(len(img_ids)):\n",
    "        labels_per_imgs.append(coco.loadAnns(coco.getAnnIds(imgIds=img_ids[i])))\n",
    "\n",
    "    img_id_w_bb = []\n",
    "    label_per_obj = []\n",
    "\n",
    "    for labels in labels_per_imgs:\n",
    "        for l in labels:\n",
    "            img_id_w_bb.append((l['id'], l['image_id'], l['bbox']))\n",
    "            label_per_obj.append(l['category_id'])\n",
    "\n",
    "    return img_ids_w_filename, img_id_w_bb, label_per_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_w_filename_train, img_id_w_bb_train, label_per_obj_train = get_coco_images_and_labels(coco_train)\n",
    "img_ids_w_filename_val, img_id_w_bb_val, label_per_obj_val = get_coco_images_and_labels(coco_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filtered dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "filtered_dataset_dir = Path('dataset/coco_top10_filtered_20250423')\n",
    "\n",
    "with open(filtered_dataset_dir / 'img_id_w_bb_train_top10_v2.pkl', 'rb') as f:\n",
    "    img_id_w_bb_train_top10_filtered = pickle.load(f)\n",
    "with open(filtered_dataset_dir / 'label_per_obj_train_top10_v2.pkl', 'rb') as f:\n",
    "    label_per_obj_train_top10_filtered = pickle.load(f)\n",
    "\n",
    "with open(filtered_dataset_dir/ 'img_id_w_bb_val_top10.pkl', 'rb') as f:\n",
    "    img_id_w_bb_val_top10 = pickle.load(f)\n",
    "with open(filtered_dataset_dir / 'label_per_obj_val_top10.pkl', 'rb') as f:\n",
    "    label_per_obj_val_top10 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151091, 391895, [359.17, 146.17, 112.45, 213.57])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_id_w_bb_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62444, 62444, 20312, 20312)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_id_w_bb_train_top10_filtered), len(label_per_obj_train_top10_filtered), len(img_id_w_bb_val_top10), len(label_per_obj_val_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to logits\n",
    "label_per_obj_train_top10_filtered_logits = np.array([LABEL_LOGITS_MAPPING[l] for l in label_per_obj_train_top10_filtered], dtype=np.int32)\n",
    "label_per_obj_val_top10_logits = np.array([LABEL_LOGITS_MAPPING[l] for l in label_per_obj_val_top10], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 44, 44, 44, 51, 44, 44, 44, 44, 44, 44, 1, 1, 51, 47, 44, 47, 47, 47, 47]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_per_obj_train_top10_filtered[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 7, 4, 4, 4, 4, 4, 4, 0, 0, 7, 5, 4, 5, 5, 5, 5],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_per_obj_train_top10_filtered_logits[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train-val-test set\n",
    "\n",
    "Note that the split will be slightly different, due to the different strategy in handling images-to-bbox relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import math\n",
    "\n",
    "class ReducedCOCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_id_w_bb:list, label_per_obj:list, img_ids_w_filename, coco_ds_name:str, transforms):\n",
    "        self.img_id_w_bb = img_id_w_bb\n",
    "        self.label_per_obj = label_per_obj\n",
    "        self.img_ids_w_filename = img_ids_w_filename\n",
    "        self.coco_ds_name = coco_ds_name\n",
    "        self.transforms = transforms\n",
    "        self.labels = LABELS\n",
    "\n",
    "        self._def_transform = None\n",
    "\n",
    "        assert self.coco_ds_name in ['train2017', 'val2017'], f\"Invalid coco dataset name: {self.coco_ds_name}\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the image id\n",
    "        ann_id, img_id, bbox = self.img_id_w_bb[idx]\n",
    "        label = self.label_per_obj[idx]\n",
    "\n",
    "        # load image\n",
    "        img_name = self.img_ids_w_filename[img_id]\n",
    "        img_path = Path(f\"dataset/coco/{self.coco_ds_name}/{img_name}\")\n",
    "        img = read_image(img_path)\n",
    "\n",
    "        # chop the image to the bbox\n",
    "        x1, y1, w, h = bbox\n",
    "        x1 = int(math.floor(x1))\n",
    "        y1 = int(math.floor(y1))\n",
    "        x2 = int(math.floor(x1 + w)) + 1\n",
    "        y2 = int(math.floor(y1 + h)) + 1\n",
    "        img = img[:, y1:y2, x1:x2]\n",
    "        \n",
    "\n",
    "        # any data augmentation (?)\n",
    "\n",
    "        # apply transforms\n",
    "        img_t, label = self._def_transform(img, label)\n",
    "\n",
    "        return img_t, label\n",
    "    \n",
    "    def get_details_from_id(self, idx):\n",
    "        ann_id, img_id, bbox = self.img_id_w_bb[idx]\n",
    "        label = self.label_per_obj[idx]\n",
    "\n",
    "        # load image\n",
    "        img_name = self.img_ids_w_filename[img_id]\n",
    "        img_path = Path(f\"dataset/coco/{self.coco_ds_name}/{img_name}\")\n",
    "        img = read_image(img_path)       \n",
    "\n",
    "        return img_path, img, bbox, label\n",
    "    \n",
    "    def set_def_transform(self, transform):\n",
    "        self._def_transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_id_w_bb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset =  62444 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "dataset_traintest = ReducedCOCODataset(\n",
    "    img_id_w_bb_train_top10_filtered,\n",
    "    label_per_obj_train_top10_filtered_logits,\n",
    "    img_ids_w_filename_train,\n",
    "    coco_ds_name='train2017',\n",
    "    transforms=None\n",
    ")\n",
    "print('length of dataset = ', len(dataset_traintest), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['person',\n",
       "  'car',\n",
       "  'chair',\n",
       "  'book',\n",
       "  'bottle',\n",
       "  'cup',\n",
       "  'dinning table',\n",
       "  'traffic light',\n",
       "  'bowl',\n",
       "  'handbag'],\n",
       " 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_traintest.labels, len(dataset_traintest.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset =  20312 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validation set\n",
    "\n",
    "dataset_val = ReducedCOCODataset(\n",
    "    img_id_w_bb_val_top10,\n",
    "    label_per_obj_val_top10_logits,\n",
    "    img_ids_w_filename_val,\n",
    "    coco_ds_name='val2017',\n",
    "    transforms=None\n",
    ")\n",
    "print('length of dataset = ', len(dataset_val), '\\n')\n",
    "# getting the image and target of the dataset\n",
    "# img, target = dataset_val[1]\n",
    "# print(img, '\\n',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36781"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_id_w_bb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20312"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_per_obj_val_top10_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script contents are moved to cnn_model_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop with hyperparameter selection\n",
    "\n",
    "we are looking for \n",
    "- batch size (16, 32, 64)\n",
    "- learning rate (5e-4, 1e-4, 5e-5)\n",
    "\n",
    "Total combinations: $3 \\times 3 = 9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_model_loader import ModelType, load_model, get_default_transforms\n",
    "\n",
    "model_type = ModelType.EFFICIENTNET_B4\n",
    "\n",
    "dataset_traintest.set_def_transform(\n",
    "    get_default_transforms(model_type)\n",
    ")\n",
    "\n",
    "dataset_val.set_def_transform(\n",
    "    get_default_transforms(model_type)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hyperparameter combinations:  3\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "BATCH_SIZE_GRID = [32]\n",
    "LR_GRID = [1e-4, 5e-5, 1e-5]\n",
    "\n",
    "MAX_EPOCHES = 15\n",
    "\n",
    "hyperparam_combs = list(product(BATCH_SIZE_GRID, LR_GRID))\n",
    "print('Total number of hyperparameter combinations: ', len(hyperparam_combs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tdy = datetime.now()\n",
    "top_model_dir = Path(f'models_coco/{str(model_type)}/{tdy.strftime(\"%Y%m%d-%H%M%S\")}/')\n",
    "if not top_model_dir.exists():\n",
    "    top_model_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create train and validation set\n",
    "train_indices, test_indices = train_test_split(list(range(len(dataset_traintest.img_id_w_bb))), test_size=0.2, random_state=42)\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(dataset_traintest, train_indices)\n",
    "dataset_test = torch.utils.data.Subset(dataset_traintest, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  return tuple(zip(*batch))\n",
    "\n",
    "def build_data_loaders(batch_size):\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    data_loader_valid = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=16,      # fixed for inference\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return data_loader_train, data_loader_test, data_loader_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Function to train the model over one epoch.\n",
    "'''\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader):\n",
    "  \n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "\n",
    "    labels_list = []\n",
    "    preds_list = []\n",
    "\n",
    "    tqdm_bar = tqdm(data_loader, total=len(data_loader))\n",
    "    for idx, data in enumerate(tqdm_bar):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        inputs = torch.stack(inputs, dim=0).to(DEVICE)\n",
    "        labels = torch.tensor(labels, dtype=torch.float64).type(torch.LongTensor).to(DEVICE)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        train_loss += loss_val\n",
    "        train_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        labels_list.append(labels)\n",
    "        preds_list.append(preds)\n",
    "        \n",
    "        tqdm_bar.set_description(desc=f\"Training Loss: {loss_val:.3f}\")\n",
    "\n",
    "    train_loss /= len(data_loader)\n",
    "    acc = float(train_corrects) / len(data_loader.dataset)\n",
    "    labels_list = torch.cat(labels_list).cpu().numpy()\n",
    "    preds_list = torch.cat(preds_list).cpu().numpy()\n",
    "    print_log(f\"Avg training Loss: {train_loss:.3f}; Accuracy: {acc:.3f}\")\n",
    "\n",
    "    return train_loss, acc, labels_list, preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, data_loader):\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "\n",
    "    labels_list = []\n",
    "    preds_list = []\n",
    "\n",
    "    tqdm_bar = tqdm(data_loader, total=len(data_loader))\n",
    "\n",
    "    for i, data in enumerate(tqdm_bar):\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = torch.stack(inputs, dim=0).to(DEVICE)\n",
    "        labels = torch.tensor(labels, dtype=torch.float64).type(torch.LongTensor).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "\n",
    "        loss_val = loss.item()\n",
    "        test_loss += loss_val\n",
    "        test_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        labels_list.append(labels)\n",
    "        preds_list.append(preds)\n",
    "\n",
    "        tqdm_bar.set_description(desc=f\"Testing Loss: {loss_val:.4f}\")\n",
    "\n",
    "    labels_list = torch.cat(labels_list).cpu().numpy()\n",
    "    preds_list = torch.cat(preds_list).cpu().numpy()\n",
    "    test_loss /= len(data_loader)\n",
    "    acc = float(test_corrects) / len(data_loader.dataset)\n",
    "    print_log(f\"Avg testing Loss: {test_loss:.3f}; Accuracy: {acc:.3f}\")\n",
    "    \n",
    "    return test_loss, acc, labels_list, preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_pred(model, data_loader, stage:str):\n",
    "    \n",
    "    labels_list = []\n",
    "    preds_list =[]\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    tqdm_bar = tqdm(data_loader, total=len(data_loader))\n",
    "    for i, data in enumerate(tqdm_bar):\n",
    "        inputs, labels = data\n",
    "\n",
    "        images = torch.stack(inputs, dim=0).to(DEVICE)\n",
    "        labels = torch.tensor(labels, dtype=torch.float64).type(torch.LongTensor).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        labels_list.append(labels)\n",
    "        preds_list.append(preds)\n",
    "\n",
    "        tqdm_bar.set_description(f\"Evaluating {stage}\")\n",
    "\n",
    "    labels_list = torch.cat(labels_list).cpu().numpy()\n",
    "    preds_list = torch.cat(preds_list).cpu().numpy()\n",
    "    \n",
    "    return labels_list, preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute acc, confusion matrix, classification report\n",
    "from helper_evaluations import compute_accuracy, compute_f1_score, compute_balanced_accuracy, compute_classification_report, compute_confusion_matrix\n",
    "\n",
    "def compute_classification_metrics(target_labels, pred_labels, target_names:list[str]):\n",
    "    \n",
    "    # compute accuracy\n",
    "    acc = compute_accuracy(target_labels, pred_labels)\n",
    "    print(\"Accuracy: \", acc)\n",
    "\n",
    "    report = compute_classification_report(target_labels, pred_labels, target_names)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # compute confusion matrix\n",
    "    compute_confusion_matrix(target_labels, pred_labels, target_names, save=False)\n",
    "\n",
    "    return target_labels, pred_labels\n",
    "\n",
    "def save_evaluations(y, y_pred, labels, model_dir, eval_stage=str):\n",
    "    \"\"\"Save the evaluation results\n",
    "    \n",
    "    eval_stage: str\n",
    "        The stage of the evaluation. It can be 'train', 'test' or 'val.\n",
    "    \"\"\"\n",
    "    # Save the accuracy score\n",
    "    accuracy = compute_accuracy(y, y_pred)\n",
    "    f1 = compute_f1_score(y, y_pred)\n",
    "    balanced_accuracy = compute_balanced_accuracy(y, y_pred)\n",
    "    print_log(f\"Accuracy [{eval_stage}]: {accuracy}; Weighted F1 [{eval_stage}]: {f1}; Weighted Accuracy [{eval_stage}]: {balanced_accuracy}\")\n",
    "    # save the scores\n",
    "    with open(model_dir / f'accuracy_{eval_stage}.txt', 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1}\\n\")\n",
    "        f.write(f\"Weighted Accuracy: {balanced_accuracy}\\n\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report = compute_classification_report(y, y_pred, labels)\n",
    "    with open(model_dir / f'classification_report_{eval_stage}.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    cm_path = model_dir / f'confusion_matrix_{eval_stage}.png'\n",
    "    compute_confusion_matrix(y, y_pred, labels, save=True, save_path=cm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopper -> w/out the need to test epoch; just need to define the patience\n",
    "# and maximum epoches for the model\n",
    "\n",
    "# definition moved to early_stopper.py\n",
    "\n",
    "from early_stopper import EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 04:01:55:195] - --------------------------------------------------\n",
      "[2025-04-26 04:01:55:195] - Batch size: 32, Learning rate: 0.0001\n",
      "[2025-04-26 04:01:55:195] - --------------------------------------------------\n",
      "[2025-04-26 04:01:55:195] - Model directory models_coco/efficientnet-b4/20250426-040154/bs_32_lr_0.0001 created\n",
      "[2025-04-26 04:01:55:568] - Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.836:   4%|▎         | 57/1562 [00:16<07:13,  3.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m model.train()\n\u001b[32m     40\u001b[39m print_log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_EPOCHES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m train_loss, train_acc, y_labels_train, y_pred_train = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m loss_dict[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     48\u001b[39m acc_dict[\u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m].append(train_acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, criterion, optimizer, data_loader)\u001b[39m\n\u001b[32m     28\u001b[39m loss.backward()\n\u001b[32m     29\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m loss_val = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m train_loss += loss_val\n\u001b[32m     33\u001b[39m train_corrects += (preds == labels).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for batch_size, lr in hyperparam_combs:\n",
    "    print_log(f'-' * 50)\n",
    "    print_log(f\"Batch size: {batch_size}, Learning rate: {lr}\")\n",
    "    print_log(f'-' * 50)\n",
    "\n",
    "    model_dir = top_model_dir/f'bs_{batch_size}_lr_{lr}'\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir(parents=True)\n",
    "        print_log(f\"Model directory {model_dir} created\")\n",
    "    \n",
    "    model_name = f\"{str(model_type)}_bs_{batch_size}_lr_{lr}\"\n",
    "\n",
    "    if Path(model_dir/model_name).exists():\n",
    "        print_log(f\"Model {model_dir/model_name} already exists, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # create dataloaders\n",
    "    data_loader_train, data_loader_test, data_loader_valid = build_data_loaders(batch_size)\n",
    "\n",
    "    # init model\n",
    "    model = load_model(model_type, out_features=len(LABELS))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # construct optimizer, learning rate scheduler etc.\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)       # reduce lr by 0.99 every epoch\n",
    "\n",
    "    # early stopping\n",
    "    # we monitor the testing accuracy\n",
    "    early_stopper = EarlyStopper(patience=5, delta=0.005, minimize=False)\n",
    "\n",
    "    loss_dict = {'train_loss': [], 'test_loss': []}\n",
    "    acc_dict = {'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    # training\n",
    "    for epoch in range(MAX_EPOCHES):\n",
    "        model.train()\n",
    "        print_log(f\"Epoch {epoch+1}/{MAX_EPOCHES}\")\n",
    "        train_loss, train_acc, y_labels_train, y_pred_train = train_one_epoch(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            data_loader_train,\n",
    "        )\n",
    "        loss_dict['train_loss'].append(train_loss)\n",
    "        acc_dict['train_acc'].append(train_acc)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # run on test set for evaluation (get test set loss)\n",
    "        with torch.no_grad():\n",
    "            test_loss, test_acc, y_labels_test, y_pred_test = evaluate(\n",
    "                model,\n",
    "                criterion,\n",
    "                data_loader_test,\n",
    "            )\n",
    "            loss_dict['test_loss'].append(test_loss)\n",
    "            acc_dict['test_acc'].append(test_acc)\n",
    "\n",
    "        early_stop = early_stopper.step(test_acc, epoch, model)\n",
    "\n",
    "        if early_stop:\n",
    "            print_log(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print_log(f\"Best epoch: {early_stopper.best_epoch+1}; Best Acc: {early_stopper.best_loss:.3f}\")\n",
    "    # get the best model\n",
    "    best_model = early_stopper.get_best_model()\n",
    "    best_epoch = early_stopper.best_epoch\n",
    "\n",
    "    # save the best model\n",
    "    model_name = model_name + f'_epoch_{best_epoch+1}.pth'\n",
    "    torch.save(best_model.state_dict(), model_dir/model_name)\n",
    "    print_log(f\"Model saved to {model_dir/model_name}\")\n",
    "\n",
    "    # save the loss dict\n",
    "    loss_dict_path = model_dir/'loss_dict.pkl'\n",
    "    with open(loss_dict_path, 'wb') as f:\n",
    "        pickle.dump(loss_dict, f)\n",
    "    print_log(f\"Loss dict saved to {loss_dict_path}\")\n",
    "\n",
    "\n",
    "    # evaluate the model on train, test and validation set\n",
    "    best_model.eval()\n",
    "\n",
    "    # train set\n",
    "    # save_evaluations(y_labels_train, y_pred_train, LABELS, model_dir, eval_stage='train')\n",
    "    # y_labels_train_path = model_dir / 'y_labels_train.pkl'\n",
    "    # y_pred_train_path = model_dir / 'y_pred_train.pkl'\n",
    "    # np.save(y_labels_train_path, y_labels_train)\n",
    "    # np.save(y_pred_train_path, y_pred_train)\n",
    "\n",
    "    y_labels_test, y_pred_test = evaluation_pred(best_model, data_loader_test, stage='test')\n",
    "    save_evaluations(y_labels_test, y_pred_test, LABELS, model_dir, eval_stage='test')\n",
    "    y_labels_test_path = model_dir / 'y_labels_test'\n",
    "    y_pred_test_path = model_dir / 'y_pred_test'\n",
    "    np.save(y_labels_test_path, y_labels_test)\n",
    "    np.save(y_pred_test_path, y_pred_test)\n",
    "\n",
    "    # save the model predictions\n",
    "    y_labels_valid, y_pred_valid = evaluation_pred(best_model, data_loader_valid, stage='validation')\n",
    "    save_evaluations(y_labels_valid, y_pred_valid, LABELS, model_dir, eval_stage='validation')\n",
    "    y_labels_valid_path = model_dir / 'y_labels_valid'\n",
    "    y_pred_valid_path = model_dir / 'y_pred_valid'\n",
    "    np.save(y_labels_valid_path, y_labels_valid)\n",
    "    np.save(y_pred_valid_path, y_pred_valid)\n",
    "\n",
    "    print_log(f\"Finished training for batch size: {batch_size}, learning rate: {lr}\")\n",
    "    print_log(f'-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained faster rcnn network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# using mobile net for faster training, while have decent accuracy\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn()\n",
    "\n",
    "n_classes = len(dataset_traintest.labels)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "FasterRCNN                                              [100, 4]                  --\n",
       "├─GeneralizedRCNNTransform: 1-1                         [8, 3, 800, 800]          --\n",
       "├─BackboneWithFPN: 1-2                                  [8, 256, 13, 13]          --\n",
       "│    └─IntermediateLayerGetter: 2-1                     [8, 960, 25, 25]          --\n",
       "│    │    └─Conv2dNormActivation: 3-1                   [8, 16, 400, 400]         (432)\n",
       "│    │    └─InvertedResidual: 3-2                       [8, 16, 400, 400]         (400)\n",
       "│    │    └─InvertedResidual: 3-3                       [8, 24, 200, 200]         (3,136)\n",
       "│    │    └─InvertedResidual: 3-4                       [8, 24, 200, 200]         (4,104)\n",
       "│    │    └─InvertedResidual: 3-5                       [8, 40, 100, 100]         (9,960)\n",
       "│    │    └─InvertedResidual: 3-6                       [8, 40, 100, 100]         (20,432)\n",
       "│    │    └─InvertedResidual: 3-7                       [8, 40, 100, 100]         (20,432)\n",
       "│    │    └─InvertedResidual: 3-8                       [8, 80, 50, 50]           30,960\n",
       "│    │    └─InvertedResidual: 3-9                       [8, 80, 50, 50]           33,800\n",
       "│    │    └─InvertedResidual: 3-10                      [8, 80, 50, 50]           31,096\n",
       "│    │    └─InvertedResidual: 3-11                      [8, 80, 50, 50]           31,096\n",
       "│    │    └─InvertedResidual: 3-12                      [8, 112, 50, 50]          212,280\n",
       "│    │    └─InvertedResidual: 3-13                      [8, 112, 50, 50]          383,208\n",
       "│    │    └─InvertedResidual: 3-14                      [8, 160, 25, 25]          426,216\n",
       "│    │    └─InvertedResidual: 3-15                      [8, 160, 25, 25]          793,200\n",
       "│    │    └─InvertedResidual: 3-16                      [8, 160, 25, 25]          793,200\n",
       "│    │    └─Conv2dNormActivation: 3-17                  [8, 960, 25, 25]          153,600\n",
       "│    └─FeaturePyramidNetwork: 2-2                       [8, 256, 13, 13]          --\n",
       "│    │    └─ModuleList: 3-20                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-21                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-20                            --                        (recursive)\n",
       "│    │    └─ModuleList: 3-21                            --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-22                      [8, 256, 25, 25]          --\n",
       "├─RegionProposalNetwork: 1-3                            [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                     [8, 15, 25, 25]           --\n",
       "│    │    └─Sequential: 3-23                            [8, 256, 25, 25]          590,080\n",
       "│    │    └─Conv2d: 3-24                                [8, 15, 25, 25]           3,855\n",
       "│    │    └─Conv2d: 3-25                                [8, 60, 25, 25]           15,420\n",
       "│    │    └─Sequential: 3-26                            [8, 256, 25, 25]          (recursive)\n",
       "│    │    └─Conv2d: 3-27                                [8, 15, 25, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-28                                [8, 60, 25, 25]           (recursive)\n",
       "│    │    └─Sequential: 3-29                            [8, 256, 13, 13]          (recursive)\n",
       "│    │    └─Conv2d: 3-30                                [8, 15, 13, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-31                                [8, 60, 13, 13]           (recursive)\n",
       "│    └─AnchorGenerator: 2-4                             [21285, 4]                --\n",
       "├─RoIHeads: 1-4                                         [100, 4]                  --\n",
       "│    └─MultiScaleRoIAlign: 2-5                          [8000, 256, 7, 7]         --\n",
       "│    └─TwoMLPHead: 2-6                                  [8000, 1024]              --\n",
       "│    │    └─Linear: 3-32                                [8000, 1024]              12,846,080\n",
       "│    │    └─Linear: 3-33                                [8000, 1024]              1,049,600\n",
       "│    └─FastRCNNPredictor: 2-7                           [8000, 11]                --\n",
       "│    │    └─Linear: 3-34                                [8000, 11]                11,275\n",
       "│    │    └─Linear: 3-35                                [8000, 44]                45,100\n",
       "=========================================================================================================\n",
       "Total params: 18,976,354\n",
       "Trainable params: 18,917,458\n",
       "Non-trainable params: 58,896\n",
       "Total mult-adds (Units.GIGABYTES): 147.57\n",
       "=========================================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 3797.62\n",
       "Params size (MB): 75.91\n",
       "Estimated Total Size (MB): 3878.34\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(8, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(dataset_traintest, train_indices)\n",
    "dataset_test = torch.utils.data.Subset(dataset_traintest, test_indices)\n",
    "# create data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_valid = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12674, 3169, 3559)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader_train.dataset), len(data_loader_test.dataset), len(data_loader_valid.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv2dNormActivation(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-1): 2 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "\n",
    "# construct optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.AdamW(params, lr=5e-3)\n",
    "# following the tutorial first. Not sure if this is the best optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001,\n",
    "                        momentum=0.9,\n",
    "                        weight_decay=0.0005)\n",
    "\n",
    "# construct learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_EPOCHS, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Function to train the model over one epoch.\n",
    "'''\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "  train_loss_list = []\n",
    "\n",
    "  tqdm_bar = tqdm(data_loader, total=len(data_loader))\n",
    "  for idx, data in enumerate(tqdm_bar):\n",
    "    optimizer.zero_grad()\n",
    "    images, targets = data\n",
    "\n",
    "    # print(targets)\n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # targets = {'boxes'=tensor, 'labels'=tensor}\n",
    "\n",
    "    losses = model(images, targets)\n",
    "\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss_val = loss.item()\n",
    "    train_loss_list.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    tqdm_bar.set_description(desc=f\"Training Loss: {loss_val:.3f}\")\n",
    "\n",
    "  return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to validate the model\n",
    "\n",
    "The losses output will be a dictionary with the following keys (and sample values):\n",
    "{'loss_classifier': tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
    " 'loss_box_reg': tensor(0.1033, device='cuda:0', grad_fn=<DivBackward0>),\n",
    " 'loss_objectness': tensor(0.1994, device='cuda:0',\n",
    "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
    " 'loss_rpn_box_reg': tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)}\n",
    "'''\n",
    "\n",
    "def evaluate(model, data_loader_test, device):\n",
    "    val_loss_list = []\n",
    "\n",
    "    tqdm_bar = tqdm(data_loader_test, total=len(data_loader_test))\n",
    "\n",
    "    for i, data in enumerate(tqdm_bar):\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses = model(images, targets)\n",
    "\n",
    "        loss = sum(loss for loss in losses.values())\n",
    "        loss_val = loss.item()\n",
    "        val_loss_list.append(loss_val)\n",
    "\n",
    "        tqdm_bar.set_description(desc=f\"Testing Loss: {loss:.4f}\")\n",
    "    return val_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to plot training and valdiation losses and save them in `output_dir'\n",
    "'''\n",
    "def plot_loss(train_loss, valid_loss):\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "\n",
    "    train_ax.plot(train_loss, color='blue')\n",
    "    train_ax.set_xlabel('Iteration')\n",
    "    train_ax.set_ylabel('Training Loss')\n",
    "\n",
    "    valid_ax.plot(valid_loss, color='red')\n",
    "    valid_ax.set_xlabel('Iteration')\n",
    "    valid_ax.set_ylabel('Validation loss')\n",
    "\n",
    "    # figure_1.savefig(f\"{OUTPUT_DIR}/train_loss.png\")\n",
    "    # figure_2.savefig(f\"{OUTPUT_DIR}/valid_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.505: 100%|██████████| 6337/6337 [03:53<00:00, 27.12it/s]\n",
      "Testing Loss: 0.1891: 100%|██████████| 1585/1585 [00:43<00:00, 36.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_dict = {'train_loss': [], 'test_loss': []}\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(\"----------Epoch {}----------\".format(epoch+1))\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_loss_list = train_one_epoch(model, optimizer, data_loader_train, device)\n",
    "    loss_dict['train_loss'].extend(train_loss_list)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Run evaluation to get losses\n",
    "    test_loss_list = evaluate(model, data_loader_test, device)\n",
    "    loss_dict['test_loss'].extend(test_loss_list)\n",
    "\n",
    "    # store the best model\n",
    "    if best_model is None or min(test_loss_list) < min(loss_dict['test_loss']):\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    break\n",
    "\n",
    "    # Svae the model ckpt after every epoch\n",
    "    # ckpt_file_name = f\"{OUTPUT_DIR}/epoch_{epoch+1}_model.pth\"\n",
    "    # torch.save({\n",
    "    #     'epoch': epoch+1,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'loss_dict': loss_dict\n",
    "    # }, ckpt_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img, _target = dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[250.8200, 168.2600, 320.9300, 233.1400],\n",
       "         [435.3500, 294.2300, 448.8100, 302.0400],\n",
       "         [447.4400, 293.9100, 459.6000, 301.5600],\n",
       "         [460.5900, 291.7100, 473.3400, 300.1600],\n",
       "         [407.0700, 287.2500, 419.7200, 297.1100],\n",
       "         [618.0600, 289.3100, 629.6600, 297.2600],\n",
       "         [512.3000, 294.0700, 533.4800, 299.6400],\n",
       "         [285.5500, 370.5600, 297.6200, 389.7700]]),\n",
       " 'labels': tensor([1, 2, 2, 2, 2, 2, 2, 1]),\n",
       " 'area': tensor([4548.7363,  105.1225,   93.0240,  107.7377,  124.7288,   92.2199,\n",
       "          117.9727,  231.8647]),\n",
       " 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'image_id': tensor([532481])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# _images = list(image.to(device) for image in images)\n",
    "_targets = [{k: v.to(device) for k, v in t.items()} for t in [_target]]\n",
    "\n",
    "prediction = model([_img.to(device)], _targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[228.5561, 162.7497, 324.4900, 276.3049],\n",
       "          [ 60.8669,  42.7334,  97.8022, 129.5400],\n",
       "          [269.1109, 163.7460, 309.8487, 258.3119],\n",
       "          [238.0428, 165.0763, 313.6662, 229.2963],\n",
       "          [ 60.8862,  45.8456,  81.3751, 123.9119],\n",
       "          [248.5014, 163.1061, 295.5199, 283.9343],\n",
       "          [ 72.5282,  50.9944,  89.5937, 127.8398],\n",
       "          [203.8169, 133.1695, 346.3714, 384.7503],\n",
       "          [ 52.4172,  43.8782, 117.0351,  99.8453],\n",
       "          [175.4869, 172.9174, 350.4914, 263.5399],\n",
       "          [ 45.7011,  34.9295, 120.2405, 134.7366],\n",
       "          [163.5344, 159.9768, 365.1031, 228.9554],\n",
       "          [260.2890, 150.8506, 333.2132, 360.5005],\n",
       "          [222.7567, 193.5051, 319.2958, 251.7576],\n",
       "          [294.0408, 186.9751, 315.8139, 245.1031],\n",
       "          [  0.0000,  44.0493, 103.7229, 124.7355]], device='cuda:0',\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], device='cuda:0'),\n",
       "  'scores': tensor([0.7640, 0.4472, 0.4222, 0.2712, 0.2498, 0.1576, 0.1518, 0.1031, 0.0911,\n",
       "          0.0837, 0.0704, 0.0697, 0.0550, 0.0520, 0.0507, 0.0501],\n",
       "         device='cuda:0', grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp61342_asm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
