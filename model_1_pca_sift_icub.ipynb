{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dc1a8d",
   "metadata": {},
   "source": [
    "The model training script for SIFT on modified iCUB World Transformation dataset\n",
    "\n",
    "20 classes, with cropped images and better cropped images from manual annotation\n",
    "\n",
    "TODO: change it to pyscript and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d44f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from helper import print_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e12e21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path('dataset/icub_custom_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c6c8065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bodylotion', 'book', 'cellphone', 'flower', 'glass', 'hairbrush', 'hairclip', 'mouse', 'mug', 'ovenglove', 'pencilcase', 'perfume', 'remote', 'ringbinder', 'soapdispenser', 'sodabottle', 'sprayer', 'squeezer', 'sunglasses', 'wallet']\n",
      "\n",
      "\n",
      "{0: 'bodylotion', 1: 'book', 2: 'cellphone', 3: 'flower', 4: 'glass', 5: 'hairbrush', 6: 'hairclip', 7: 'mouse', 8: 'mug', 9: 'ovenglove', 10: 'pencilcase', 11: 'perfume', 12: 'remote', 13: 'ringbinder', 14: 'soapdispenser', 15: 'sodabottle', 16: 'sprayer', 17: 'squeezer', 18: 'sunglasses', 19: 'wallet'}\n",
      "\n",
      "\n",
      "{'bodylotion': 0, 'book': 1, 'cellphone': 2, 'flower': 3, 'glass': 4, 'hairbrush': 5, 'hairclip': 6, 'mouse': 7, 'mug': 8, 'ovenglove': 9, 'pencilcase': 10, 'perfume': 11, 'remote': 12, 'ringbinder': 13, 'soapdispenser': 14, 'sodabottle': 15, 'sprayer': 16, 'squeezer': 17, 'sunglasses': 18, 'wallet': 19}\n"
     ]
    }
   ],
   "source": [
    "cats = sorted([cat for cat in os.listdir(dataset_dir) if os.path.isdir(dataset_dir / cat)])\n",
    "LABEL_TO_CAT = {i:cat for i, cat in enumerate(cats)}\n",
    "CAT_TO_LABEL = {cat:i for i, cat in enumerate(cats)}\n",
    "LABELS = cats\n",
    "print(cats); print(); print()\n",
    "print(LABEL_TO_CAT); print(); print()\n",
    "print(CAT_TO_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e800dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train-test set and validation set\n",
    "\n",
    "train_test_set_path = dataset_dir / 'train_test_set.pkl'\n",
    "train_test_set_label_path = dataset_dir / 'train_test_set_labels.pkl'\n",
    "eval_set_path = dataset_dir / 'eval_set.pkl'\n",
    "eval_set_label_path = dataset_dir / 'eval_set_labels.pkl'\n",
    "\n",
    "if not train_test_set_path.exists():\n",
    "    raise FileNotFoundError(f\"Train-test set not found at {train_test_set_path}\")\n",
    "if not train_test_set_label_path.exists():\n",
    "    raise FileNotFoundError(f\"Train-test set labels not found at {train_test_set_label_path}\")\n",
    "if not eval_set_path.exists():\n",
    "    raise FileNotFoundError(f\"Validation set not found at {eval_set_path}\")\n",
    "\n",
    "\n",
    "train_test_set = pickle.load(open(train_test_set_path, 'rb'))\n",
    "train_test_set_label = pickle.load(open(train_test_set_label_path, 'rb'))\n",
    "eval_set = pickle.load(open(eval_set_path, 'rb'))\n",
    "eval_set_label = pickle.load(open(eval_set_label_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "244ec34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003761.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003827.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003785.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003835.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003778.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003814.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003886.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003852.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003845.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003782.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003824.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003806.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003815.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003767.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003869.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003853.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003776.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003745.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003773.jpg'),\n",
       " PosixPath('dataset/icub_custom_subset/bodylotion/bodylotion5/day1/00003885.jpg')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_set[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb721a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_set_label[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b93fe277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 19, 19, 19, 19, 19, 19, 19, 19, 19]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set_label[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ace5b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf267fb",
   "metadata": {},
   "source": [
    "Create train-test split\n",
    "\n",
    "Training-test split has ratio 4:1; with stratify consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5bff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the train-test set into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_test_set, train_test_set_label, test_size=0.2, random_state=42, stratify=train_test_set_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6866a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 1. First, prepare your data for the datasets library\n",
    "def prepare_dataset_list(X):\n",
    "    # Create a lightweight dictionary containing only metadata (not images)\n",
    "\n",
    "    dataset_list = []\n",
    "    \n",
    "    for sample in X:\n",
    "        per_img_dict = {}\n",
    "        per_img_dict[\"image_id\"] = '/'.join(sample.parts[2:])\n",
    "        per_img_dict[\"image_path\"] = sample\n",
    "\n",
    "        dataset_list.append(per_img_dict)\n",
    "        \n",
    "    return dataset_list\n",
    "\n",
    "# 2. Define the SIFT processing function\n",
    "\n",
    "def process_image_with_sift(example):\n",
    "    \"\"\"Process a single image, extracting SIFT features\"\"\"\n",
    "    # Load image only when needed\n",
    "    img_path = example['image_path']\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply SIFT\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_gray, None)\n",
    "    \n",
    "    # Convert keypoints to serializable format\n",
    "    serialized_keypoints = []\n",
    "    for kp in keypoints:\n",
    "        serialized_keypoints.append({\n",
    "            'x': float(kp.pt[0]),\n",
    "            'y': float(kp.pt[1]), \n",
    "            'size': float(kp.size),\n",
    "            'angle': float(kp.angle),\n",
    "            'response': float(kp.response),\n",
    "            'octave': int(kp.octave)\n",
    "        })\n",
    "    \n",
    "    # Return only the features, together with the image_id, bbox, and filename\n",
    "    # but not the image (pixel) itself\n",
    "    return {\n",
    "        'image_id': example['image_id'],\n",
    "        'image_path': example['image_path'],\n",
    "        'keypoints': serialized_keypoints,\n",
    "        'descriptors': descriptors if descriptors is not None else np.array([])\n",
    "    }\n",
    "\n",
    "def create_sift_dataset(X):\n",
    "    # Create the dataset dictionary\n",
    "    dataset_list_init = prepare_dataset_list(X)\n",
    "\n",
    "    dataset_list = []\n",
    "\n",
    "    with tqdm(total=len(dataset_list_init)) as pbar:\n",
    "        pbar.set_description(\"Extracting SIFT features\")\n",
    "\n",
    "        for i, example in enumerate(dataset_list_init):\n",
    "            # Process using (optional) multi-processing\n",
    "            processed_example = process_image_with_sift(example)\n",
    "            dataset_list.append(processed_example)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7f57e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIFT features: 100%|██████████| 10400/10400 [00:50<00:00, 204.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset is created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "sift_dataset_train_path = dataset_dir.joinpath('sift_dataset_train.pkl')\n",
    "\n",
    "if sift_dataset_train_path.exists():\n",
    "    sift_dataset_train = pickle.load(open(sift_dataset_train_path, 'rb'))\n",
    "\n",
    "    print(\"Training dataset already exists. Loading from disk...\")\n",
    "\n",
    "else:\n",
    "    # Create the dataset\n",
    "    sift_dataset_train = create_sift_dataset(X_train)\n",
    "\n",
    "    # Save the dataset to disk\n",
    "    pickle.dump(sift_dataset_train, open(sift_dataset_train_path, 'wb'))\n",
    "\n",
    "    print(\"Training dataset is created and saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dbd0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIFT features: 100%|██████████| 2600/2600 [00:11<00:00, 234.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset is created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "sift_dataset_test_path = dataset_dir.joinpath('sift_dataset_test.pkl')\n",
    "\n",
    "if sift_dataset_test_path.exists():\n",
    "    sift_dataset_test = pickle.load(open(sift_dataset_test_path, 'rb'))\n",
    "\n",
    "    print(\"Testing dataset already exists. Loading from disk...\")\n",
    "\n",
    "else:\n",
    "    # Create the dataset\n",
    "    sift_dataset_test = create_sift_dataset(X_test)\n",
    "\n",
    "    # Save the dataset to disk\n",
    "    pickle.dump(sift_dataset_test, open(sift_dataset_test_path, 'wb'))\n",
    "\n",
    "    print(\"Testing dataset is created and saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ad55b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIFT features: 100%|██████████| 3000/3000 [00:15<00:00, 189.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset is created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "sift_dataset_val_path = dataset_dir.joinpath('sift_dataset_val.pkl')\n",
    "if sift_dataset_val_path.exists():\n",
    "    sift_dataset_val = pickle.load(open(sift_dataset_val_path, 'rb'))\n",
    "\n",
    "    print(\"Validation dataset already exists. Loading from disk...\")\n",
    "else:\n",
    "    # create the dataset\n",
    "    sift_dataset_val = create_sift_dataset(eval_set)\n",
    "\n",
    "    # try to save the dataset\n",
    "    pickle.dump(sift_dataset_val, open(sift_dataset_val_path, 'wb'))\n",
    "\n",
    "    print(\"Validation dataset is created and saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bdb9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_descriptors(sift_dataset):\n",
    "\n",
    "    all_descriptors = []\n",
    "    for example in sift_dataset:\n",
    "        # check if descriptors is not empty\n",
    "        if example['descriptors'].ndim < 2:\n",
    "            continue\n",
    "\n",
    "        all_descriptors.append(example['descriptors'])\n",
    "    # convert to numpy array\n",
    "    all_descriptors_np = np.concatenate(all_descriptors, axis=0)\n",
    "\n",
    "    return all_descriptors_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ecd4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All descriptors for training dataset is created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# grab all descriptors (write them to memory, as KMeans have no incremental fit)\n",
    "all_descriptors_train_path = dataset_dir.joinpath('all_descriptors_train.npy')\n",
    "\n",
    "if all_descriptors_train_path.exists():\n",
    "    all_descriptors_np_train = np.load(all_descriptors_train_path)\n",
    "    \n",
    "    print(\"All descriptors for training dataset already exists. Loading from disk...\")\n",
    "\n",
    "else:\n",
    "    all_descriptors_np_train = load_all_descriptors(sift_dataset_train)\n",
    "    \n",
    "    # save to disk\n",
    "    np.save(all_descriptors_train_path, all_descriptors_np_train)\n",
    "\n",
    "    print(\"All descriptors for training dataset is created and saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca7a5927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224006, 128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_descriptors_np_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694a6b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12244680",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "283f0f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DESCIPTORS_DIM = len(sift_dataset_train[0]['descriptors'][0])\n",
    "DESCIPTORS_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7fec5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3ebc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "K_GRID = [20, 40, 80, 160, 320, 640]              # number of visual words\n",
    "PCA_N_COMPONENTS_GRID = [20, 50, 128]       # 128 is the default for SIFT -> no PCA reduction. Also this suits the ratio in natural log\n",
    "\n",
    "hyperparam_comb = list(product(K_GRID, PCA_N_COMPONENTS_GRID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a39f23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tdy = datetime.now()\n",
    "top_model_dir = Path(f'models_icub/PCA-SIFT/{tdy.strftime(\"%Y%m%d-%H%M%S\")}/')\n",
    "if not top_model_dir.exists():\n",
    "    top_model_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bf7e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1080107623, 2134142190, 1119297872, 1763205945, 1809128349, 1948638083, 1002935388, 2094951353, 603088789, 1010047835, 901855390, 1880209804, 554896593, 17159677, 130165681, 1876891577, 993785595, 1266814577]\n",
      "[1180331630, 934662903, 1023666597, 212999058, 75856525, 877998333, 111977320, 654038152, 193951144, 2018043230, 2131434013, 398609110, 1239425448, 565126210, 1699739114, 1023394739, 1894816029, 36511769]\n",
      "[28218918, 839809952, 1033947849, 1670555220, 485060760, 1971049986, 1405797208, 1568808299, 1448332959, 2043261654, 494949694, 2012750888, 426170715, 1502004243, 1718499774, 2139082671, 1506478259, 1425286422]\n"
     ]
    }
   ],
   "source": [
    "# base on today's date, set the random seed\n",
    "# then randomly create random seed for the each K-mean model\n",
    "import random\n",
    "\n",
    "random.seed(int(tdy.strftime(\"%Y%m%d\")))\n",
    "\n",
    "# create a random seed for each K-mean model\n",
    "seed_for_KMeans = [random.randint(0, 2147483647) for _ in range(len(hyperparam_comb))]   # 2^32 - 1\n",
    "print(seed_for_KMeans)\n",
    "\n",
    "# create a random seed for each RBF kernel approximation\n",
    "seed_for_RBF = [random.randint(0, 2147483647) for _ in range(len(hyperparam_comb))]   # 2^32 - 1\n",
    "print(seed_for_RBF)\n",
    "\n",
    "# create a random seed for each LinearSVC model\n",
    "seed_for_LinearSVC = [random.randint(0, 2147483647) for _ in range(len(hyperparam_comb))]   # 2^32 - 1\n",
    "print(seed_for_LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf9ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def PCA_training(X, n_components):\n",
    "    \"\"\"Apply PCA to the data\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0153c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def KMeans_training(X, n_clusters, random_state):\n",
    "    \"\"\"Apply KMeans to the data\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    kmeans.fit(X)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfe450cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation test-set function\n",
    "\n",
    "# extend the dataset with the cluster id (equivalent to vector quantization)\n",
    "def _assign_cluster_id(example, pca, kmeans, pca_n_components):\n",
    "    \"\"\"Assign cluster id to each keypoint based on the closest cluster center\"\"\"\n",
    "    # Update the example with the cluster ids\n",
    "\n",
    "    des = np.array(example['descriptors'])\n",
    "    # check if descriptors are empty\n",
    "    if des.size == 0:\n",
    "        example['cluster_ids'] = np.array([], dtype=np.int32)\n",
    "        return example\n",
    "    \n",
    "    # apply PCA to the descriptors\n",
    "    if pca_n_components < DESCIPTORS_DIM:\n",
    "        red_des = pca.transform(des)\n",
    "    else:\n",
    "        red_des = des\n",
    "\n",
    "    # early return if transformed descriptors are empty\n",
    "    if red_des.size == 0:\n",
    "        example['cluster_ids'] = np.array([], dtype=np.int32)\n",
    "        return example\n",
    "\n",
    "    if red_des.ndim == 1:\n",
    "        example['cluster_ids'] = kmeans.predict(red_des.reshape(1, -1))\n",
    "    else:\n",
    "        example['cluster_ids'] = kmeans.predict(red_des)\n",
    "    \n",
    "    return example\n",
    "\n",
    "def assign_cluster_id(sift_dataset, pca, kmeans, pca_n_components):\n",
    "    # Apply the filter to the dataset\n",
    "    with tqdm(total=len(sift_dataset)) as pbar:\n",
    "        pbar.set_description(\"Assigning cluster ids to keypoints\")\n",
    "        for idx, example in enumerate(sift_dataset):\n",
    "            sift_dataset[idx] = _assign_cluster_id(example, pca, kmeans, pca_n_components)\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8d5aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the cluster ids\n",
    "# that will be used to compute TF-IDF\n",
    "\n",
    "def _create_histogram(example, K):\n",
    "    \"\"\"Create a histogram of cluster ids\"\"\"\n",
    "\n",
    "    # early exit if descriptors are empty -> cluster_ids will be empty too\n",
    "    if len(example['descriptors']) == 0:\n",
    "        example['histogram'] = np.array([[]], dtype=np.int64)\n",
    "        return example\n",
    "\n",
    "    hist, _ = np.histogram(example['cluster_ids'], bins=np.arange(K + 1))\n",
    "    \n",
    "    example['histogram'] = hist.reshape(-1, K)\n",
    "\n",
    "    return example\n",
    "\n",
    "def create_histogram(sift_dataset, K):\n",
    "    # Apply the histogram function to the dataset\n",
    "    with tqdm(total=len(sift_dataset)) as pbar:\n",
    "        pbar.set_description(\"Creating histogram of cluster ids\")\n",
    "        for idx, example in enumerate(sift_dataset):\n",
    "            sift_dataset[idx] = _create_histogram(example, K)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "719b0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def create_tfidf_matrix(sift_dataset):\n",
    "    \"\"\"Create a TF-IDF matrix from the histograms\"\"\"\n",
    "    tfidf = TfidfTransformer()\n",
    "\n",
    "    # grab all non-empty histograms and concat them to a very large 2D array\n",
    "    histograms = np.array([example['histogram'] for idx, example in enumerate(sift_dataset) if len(example['histogram'][0]) > 0])\n",
    "    # reshape\n",
    "    histograms = histograms.reshape(histograms.shape[0], -1)\n",
    "    \n",
    "    # Compute the TF-IDF matrix\n",
    "    tfidf_matrix = tfidf.fit_transform(histograms)\n",
    "    \n",
    "    return tfidf, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de275cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sift_dataset, y, pca, kmeans, tfidf, rbf, svm, K, PCA_N_COMPONENT):\n",
    "    # Apply the filter to the dataset\n",
    "    assign_cluster_id(sift_dataset, pca, kmeans, PCA_N_COMPONENT)\n",
    "\n",
    "    # Apply the histogram function to the dataset\n",
    "    create_histogram(sift_dataset, K)\n",
    "\n",
    "    des_histo = np.concatenate(\n",
    "        [example['histogram'] for idx, example in enumerate(sift_dataset) if len(example['histogram'][0]) > 0],\n",
    "        axis=0\n",
    "    )\n",
    "    des_histo = des_histo.reshape(des_histo.shape[0], -1)\n",
    "\n",
    "    # Convert the list of descriptors to TF-IDF representation\n",
    "    tfidf_matrix = tfidf.transform(des_histo)\n",
    "\n",
    "    y_filtered = [y[i] for i, example in enumerate(sift_dataset) if len(example['histogram'][0]) > 0]\n",
    "    y_filtered = np.array(y_filtered)\n",
    "\n",
    "    # Predict the labels using the RBF + SVM model\n",
    "    y_pred = svm.predict(\n",
    "        rbf.transform(tfidf_matrix)\n",
    "    )\n",
    "\n",
    "    return y_filtered, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75f14619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_evaluations import compute_accuracy, compute_f1_score, compute_balanced_accuracy, compute_classification_report, compute_confusion_matrix\n",
    "\n",
    "def save_evaluations(y, y_pred, labels, model_dir, eval_stage=str):\n",
    "    \"\"\"Save the evaluation results\n",
    "    \n",
    "    eval_stage: str\n",
    "        The stage of the evaluation. It can be 'train', 'test' or 'val.\n",
    "    \"\"\"\n",
    "    # Save the accuracy score\n",
    "    accuracy = compute_accuracy(y, y_pred)\n",
    "    f1 = compute_f1_score(y, y_pred)\n",
    "    balanced_accuracy = compute_balanced_accuracy(y, y_pred)\n",
    "    print_log(f\"Accuracy [{eval_stage}]: {accuracy}; Weighted F1 [{eval_stage}]: {f1}; Weighted Accuracy [{eval_stage}]: {balanced_accuracy}\")\n",
    "    # save the scores\n",
    "    with open(model_dir / f'accuracy_{eval_stage}.txt', 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1}\\n\")\n",
    "        f.write(f\"Weighted Accuracy: {balanced_accuracy}\\n\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report = compute_classification_report(y, y_pred, labels)\n",
    "    with open(model_dir / f'classification_report_{eval_stage}.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    cm_path = model_dir / f'confusion_matrix_{eval_stage}.png'\n",
    "    compute_confusion_matrix(y, y_pred, labels, save=True, save_path=cm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf23b88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:07:20:586] - --------------------------------------------------\n",
      "[2025-04-26 01:07:20:586] - K: 20, PCA_N_COMPONENT: 20\n",
      "[2025-04-26 01:07:20:586] - --------------------------------------------------\n",
      "[2025-04-26 01:07:21:046] - PCA-SIFT_PCA_PCA-N_20.pkl model saved.\n",
      "[2025-04-26 01:07:21:047] - PCA-SIFT_KMeans_PCA-N_20_KMeans-K_20.pkl model exists. Reload the KMeans model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning cluster ids to keypoints: 100%|██████████| 10400/10400 [00:01<00:00, 7069.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:07:22:522] - Cluster ids assigned to the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating histogram of cluster ids: 100%|██████████| 10400/10400 [00:00<00:00, 98415.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:07:22:629] - Histogram of cluster ids created.\n",
      "[2025-04-26 01:07:22:638] - PCA-SIFT_TFIDF_PCA-N_20_KMeans-K_20.pkl model saved.\n",
      "[2025-04-26 01:08:18:809] - RBF Kernel and LinearSVC are trained with 20 clusters and PCA 20 components.\n",
      "[2025-04-26 01:08:18:809] - PCA-SIFT_RBF_PCA-N_20_KMeans-K_20.pkl model saved.\n",
      "[2025-04-26 01:08:18:809] - PCA-SIFT_SVM-SGD_PCA-N_20_KMeans-K_20.pkl model saved.\n",
      "[2025-04-26 01:08:18:827] - Accuracy [train]: 0.3523076923076923; Weighted F1 [train]: 0.3414815596055272; Weighted Accuracy [train]: 0.3523076923076923\n",
      "[2025-04-26 01:08:19:276] - Evaluation on train set done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning cluster ids to keypoints: 100%|██████████| 2600/2600 [00:00<00:00, 6557.58it/s]\n",
      "Creating histogram of cluster ids: 100%|██████████| 2600/2600 [00:00<00:00, 102729.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:08:19:759] - Accuracy [test]: 0.28384615384615386; Weighted F1 [test]: 0.26739976600859094; Weighted Accuracy [test]: 0.28384615384615386\n",
      "[2025-04-26 01:08:20:304] - Evaluation on test set done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning cluster ids to keypoints: 100%|██████████| 3000/3000 [00:00<00:00, 7939.57it/s]\n",
      "Creating histogram of cluster ids: 100%|██████████| 3000/3000 [00:00<00:00, 106857.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:08:20:761] - Accuracy [val]: 0.282; Weighted F1 [val]: 0.2665269649465164; Weighted Accuracy [val]: 0.28200000000000003\n",
      "[2025-04-26 01:08:21:175] - Evaluation on validation set done.\n",
      "[2025-04-26 01:08:21:175] - Finished training and evaluation for K=20 and PCA_N_COMPONENT=20.\n",
      "[2025-04-26 01:08:21:175] - --------------------------------------------------\n",
      "[2025-04-26 01:08:21:175] - --------------------------------------------------\n",
      "[2025-04-26 01:08:21:175] - K: 20, PCA_N_COMPONENT: 50\n",
      "[2025-04-26 01:08:21:175] - --------------------------------------------------\n",
      "[2025-04-26 01:08:21:175] - Model directory models_icub/PCA-SIFT/20250426-010657/KMeans_20_PCA_50 created.\n",
      "[2025-04-26 01:08:21:866] - PCA-SIFT_PCA_PCA-N_50.pkl model saved.\n",
      "[2025-04-26 01:08:30:718] - PCA-SIFT_KMeans_PCA-N_50_KMeans-K_20.pkl model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning cluster ids to keypoints: 100%|██████████| 10400/10400 [00:12<00:00, 822.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:08:43:363] - Cluster ids assigned to the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating histogram of cluster ids: 100%|██████████| 10400/10400 [00:00<00:00, 87286.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:08:43:484] - Histogram of cluster ids created.\n",
      "[2025-04-26 01:08:43:493] - PCA-SIFT_TFIDF_PCA-N_50_KMeans-K_20.pkl model saved.\n",
      "[2025-04-26 01:09:41:971] - RBF Kernel and LinearSVC are trained with 20 clusters and PCA 50 components.\n",
      "[2025-04-26 01:09:41:972] - PCA-SIFT_RBF_PCA-N_50_KMeans-K_20.pkl model saved.\n",
      "[2025-04-26 01:09:41:972] - PCA-SIFT_SVM-SGD_PCA-N_50_KMeans-K_20.pkl model saved.\n",
      "[2025-04-26 01:09:41:990] - Accuracy [train]: 0.35134615384615386; Weighted F1 [train]: 0.3404734767561501; Weighted Accuracy [train]: 0.35134615384615386\n",
      "[2025-04-26 01:09:42:392] - Evaluation on train set done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning cluster ids to keypoints: 100%|██████████| 2600/2600 [00:03<00:00, 737.55it/s] \n",
      "Creating histogram of cluster ids: 100%|██████████| 2600/2600 [00:00<00:00, 59676.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:09:46:010] - Accuracy [test]: 0.29115384615384615; Weighted F1 [test]: 0.274273937867217; Weighted Accuracy [test]: 0.2911538461538462\n",
      "[2025-04-26 01:09:46:589] - Evaluation on test set done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning cluster ids to keypoints: 100%|██████████| 3000/3000 [00:02<00:00, 1046.79it/s]\n",
      "Creating histogram of cluster ids: 100%|██████████| 3000/3000 [00:00<00:00, 62606.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 01:09:49:572] - Accuracy [val]: 0.27666666666666667; Weighted F1 [val]: 0.26010968456755124; Weighted Accuracy [val]: 0.27666666666666667\n",
      "[2025-04-26 01:09:49:978] - Evaluation on validation set done.\n",
      "[2025-04-26 01:09:49:979] - Finished training and evaluation for K=20 and PCA_N_COMPONENT=50.\n",
      "[2025-04-26 01:09:49:979] - --------------------------------------------------\n",
      "[2025-04-26 01:09:49:979] - --------------------------------------------------\n",
      "[2025-04-26 01:09:49:979] - K: 20, PCA_N_COMPONENT: 128\n",
      "[2025-04-26 01:09:49:979] - --------------------------------------------------\n",
      "[2025-04-26 01:09:49:979] - Model directory models_icub/PCA-SIFT/20250426-010657/KMeans_20_PCA_128 created.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m     kmeans = pickle.load(\u001b[38;5;28mopen\u001b[39m(model_dir / kmeans_model_name, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     kmeans = \u001b[43mKMeans_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_descriptors_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed_for_KMeans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# save the KMeans model\u001b[39;00m\n\u001b[32m     48\u001b[39m     pickle.dump(kmeans, \u001b[38;5;28mopen\u001b[39m(model_dir / kmeans_model_name, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mKMeans_training\u001b[39m\u001b[34m(X, n_clusters, random_state)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Apply KMeans to the data\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m kmeans\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1508\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1504\u001b[39m best_inertia, best_labels = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._n_init):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1508\u001b[39m     centers_init = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_centroids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1513\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m   1516\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitialization complete\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1020\u001b[39m, in \u001b[36m_BaseKMeans._init_centroids\u001b[39m\u001b[34m(self, X, x_squared_norms, init, random_state, sample_weight, init_size, n_centroids)\u001b[39m\n\u001b[32m   1017\u001b[39m     sample_weight = sample_weight[init_indices]\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init == \u001b[33m\"\u001b[39m\u001b[33mk-means++\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     centers, _ = \u001b[43m_kmeans_plusplus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init == \u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1028\u001b[39m     seeds = random_state.choice(\n\u001b[32m   1029\u001b[39m         n_samples,\n\u001b[32m   1030\u001b[39m         size=n_clusters,\n\u001b[32m   1031\u001b[39m         replace=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1032\u001b[39m         p=sample_weight / sample_weight.sum(),\n\u001b[32m   1033\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:258\u001b[39m, in \u001b[36m_kmeans_plusplus\u001b[39m\u001b[34m(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\u001b[39m\n\u001b[32m    255\u001b[39m np.clip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq.size - \u001b[32m1\u001b[39m, out=candidate_ids)\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m distance_to_candidates = \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    260\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[32m    263\u001b[39m np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:404\u001b[39m, in \u001b[36m_euclidean_distances\u001b[39m\u001b[34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[39m\n\u001b[32m    399\u001b[39m         YY = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == np.float32 \u001b[38;5;129;01mor\u001b[39;00m Y.dtype == np.float32:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     distances = \u001b[43m_euclidean_distances_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[32m    407\u001b[39m     distances = -\u001b[32m2\u001b[39m * safe_sparse_dot(X, Y.T, dense_output=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/comp61342_asm/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:610\u001b[39m, in \u001b[36m_euclidean_distances_upcast\u001b[39m\u001b[34m(X, XX, Y, YY, batch_size)\u001b[39m\n\u001b[32m    607\u001b[39m     d = distances[y_slice, x_slice].T\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     Y_chunk = Y[y_slice].astype(np.float64)\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m YY \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    612\u001b[39m         YY_chunk = row_norms(Y_chunk, squared=\u001b[38;5;28;01mTrue\u001b[39;00m)[np.newaxis, :]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "for i, (K, PCA_N_COMPONENT) in enumerate(hyperparam_comb):\n",
    "    print_log(f'-' * 50)\n",
    "    print_log(f'K: {K}, PCA_N_COMPONENT: {PCA_N_COMPONENT}')\n",
    "    print_log(f'-' * 50)\n",
    "\n",
    "    \n",
    "    # create a directory for each model\n",
    "    model_dir = top_model_dir / f'KMeans_{K}_PCA_{PCA_N_COMPONENT}'\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir(parents=True)\n",
    "        print_log(f'Model directory {model_dir} created.')\n",
    "    \n",
    "    pca_model_name = f'PCA-SIFT_PCA_PCA-N_{PCA_N_COMPONENT}' + '.pkl'\n",
    "    kmeans_model_name = f'PCA-SIFT_KMeans_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "    tfidf_model_name = f'PCA-SIFT_TFIDF_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "    rbf_model_name = f'PCA-SIFT_RBF_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}' + '.pkl'\n",
    "    svm_model_name = f'PCA-SIFT_SVM-SGD_PCA-N_{PCA_N_COMPONENT}_KMeans-K_{K}.pkl'\n",
    "\n",
    "    # check if the SVM model already exists\n",
    "    # if exists -> skip that\n",
    "    if (model_dir / svm_model_name).exists():\n",
    "        print_log(f'{svm_model_name} model exists. Skip this model.')\n",
    "        continue\n",
    "    \n",
    "    pca = None\n",
    "\n",
    "    if PCA_N_COMPONENT < DESCIPTORS_DIM:\n",
    "        # apply PCA\n",
    "        pca, all_descriptors_pca = PCA_training(all_descriptors_np_train, n_components=PCA_N_COMPONENT)\n",
    "        # save the PCA model\n",
    "        pickle.dump(pca, open(model_dir / pca_model_name, 'wb'))\n",
    "        print_log(f'{pca_model_name} model saved.')\n",
    "    else:\n",
    "        # no PCA needed\n",
    "        all_descriptors_pca = all_descriptors_np_train\n",
    "\n",
    "    # apply KMeans\n",
    "    # check if the KMeans model already exists\n",
    "    if (model_dir / kmeans_model_name).exists():\n",
    "        print_log(f'{kmeans_model_name} model exists. Reload the KMeans model.')\n",
    "        kmeans = pickle.load(open(model_dir / kmeans_model_name, 'rb'))\n",
    "    else:\n",
    "        kmeans = KMeans_training(all_descriptors_pca, n_clusters=K, random_state=seed_for_KMeans[i])\n",
    "        # save the KMeans model\n",
    "        pickle.dump(kmeans, open(model_dir / kmeans_model_name, 'wb'))\n",
    "        print_log(f'{kmeans_model_name} model saved.')\n",
    "\n",
    "    # apply the cluster id to the dataset\n",
    "    assign_cluster_id(sift_dataset_train, pca, kmeans, PCA_N_COMPONENT)\n",
    "    print_log(f'Cluster ids assigned to the dataset.')\n",
    "\n",
    "    # Apply the histogram function to the dataset\n",
    "    create_histogram(sift_dataset_train, K)\n",
    "    print_log(f'Histogram of cluster ids created.')\n",
    "    \n",
    "    # create TF-IDF BoVW representation\n",
    "    tfidf, tfidf_matrix = create_tfidf_matrix(sift_dataset_train)\n",
    "    # save the TF-IDF transformer\n",
    "    pickle.dump(tfidf, open(model_dir / tfidf_model_name, 'wb'))\n",
    "    print_log(f'{tfidf_model_name} model saved.')\n",
    "\n",
    "    # note that there are images with no histograms\n",
    "    # need to filter them out\n",
    "    y_train_filtered = [y_train[i] for i, example in enumerate(sift_dataset_train) if len(example['histogram'][0]) > 0]\n",
    "    y_train_filtered = np.array(y_train_filtered)\n",
    "\n",
    "    # SVM (either SGD, or SVC). The later requires ~40min per model\n",
    "    # 0425 update: use RBF kernel + LinearSVC for support to non-linearity\n",
    "    # higher n_components -> closer to SVC\n",
    "    # takes around 2-3 mins\n",
    "    rbf = RBFSampler(gamma=1, n_components=1000, random_state=seed_for_RBF[i])\n",
    "    X_features = rbf.fit_transform(tfidf_matrix)\n",
    "    # use LinearSVC for faster training\n",
    "    svm = LinearSVC(tol=1e-6, C=1.0, random_state=seed_for_LinearSVC[i])\n",
    "    svm.fit(X_features, y_train_filtered)\n",
    "\n",
    "    print_log(f'RBF Kernel and LinearSVC are trained with {K} clusters and PCA {PCA_N_COMPONENT} components.')\n",
    "    # save the model\n",
    "    pickle.dump(rbf, open(model_dir / rbf_model_name, 'wb'))\n",
    "    pickle.dump(svm, open(model_dir / svm_model_name, 'wb'))\n",
    "    print_log(f'{rbf_model_name} model saved.')\n",
    "    print_log(f'{svm_model_name} model saved.')\n",
    "\n",
    "\n",
    "    # evaluate on both train and test set\n",
    "    # evaluate on train set\n",
    "    y_pred_train = svm.predict(X_features)\n",
    "    save_evaluations(y_train_filtered, y_pred_train, labels=LABELS, model_dir=model_dir, eval_stage='train')\n",
    "    print_log(f'Evaluation on train set done.')\n",
    "\n",
    "    # save the prediction result for future use (create further evaluation)\n",
    "    y_train_filtered_path = model_dir / f'y_train_filtered.npy'\n",
    "    y_pred_train_path = model_dir / f'y_pred_train.npy'\n",
    "    np.save(y_train_filtered_path, y_train_filtered)\n",
    "    np.save(y_pred_train_path, y_pred_train)\n",
    "\n",
    "    \n",
    "    # evaluate on test set\n",
    "    y_test_filtered, y_pred_test = evaluate_model(sift_dataset_test, y_test, pca, kmeans, tfidf, rbf, svm, K, PCA_N_COMPONENT)\n",
    "    save_evaluations(y_test_filtered, y_pred_test, labels=LABELS, model_dir=model_dir, eval_stage='test')\n",
    "    print_log(f'Evaluation on test set done.')\n",
    "\n",
    "    # save the prediction result for future use (create further evaluation)\n",
    "    y_test_filtered_path = model_dir / f'y_test_filtered.npy'\n",
    "    y_pred_test_path = model_dir / f'y_pred_test.npy'\n",
    "    np.save(y_test_filtered_path, y_test_filtered)\n",
    "    np.save(y_pred_test_path, y_pred_test)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    y_val_filtered, y_pred_val = evaluate_model(sift_dataset_val, eval_set_label, pca, kmeans, tfidf, rbf, svm, K, PCA_N_COMPONENT)\n",
    "    save_evaluations(y_val_filtered, y_pred_val, labels=LABELS,  model_dir=model_dir, eval_stage='val')\n",
    "    print_log(f'Evaluation on validation set done.')\n",
    "\n",
    "    # save the prediction result for future use (create further evaluation)\n",
    "    y_val_filtered_path = model_dir / f'y_val_filtered.npy'\n",
    "    y_pred_val_path = model_dir / f'y_pred_val.npy'\n",
    "    np.save(y_val_filtered_path, y_val_filtered)\n",
    "    np.save(y_pred_val_path, y_pred_val)\n",
    "\n",
    "    print_log(f'Finished training and evaluation for K={K} and PCA_N_COMPONENT={PCA_N_COMPONENT}.')\n",
    "    print_log(f'-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ce4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07af09ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp61342_asm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
